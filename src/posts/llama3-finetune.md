---
slug: llama3-finetune
title: '–§–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥ Llama 3 8B: –ü–æ–ª–Ω—ã–π –≥–∞–π–¥ (Unsloth + PyTorch)'
description: '–ü–æ—à–∞–≥–æ–≤–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –∫–∞–∫ –¥–æ–æ–±—É—á–∏—Ç—å Llama 3 –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 16GB VRAM.'
date: '2026-01-04'
tags: ['Tutorial', 'Llama 3', 'Code', 'Unsloth']
# image: '/blog/llama3-finetune/cover.jpg'
tldr:
  - '–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É Unsloth –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤ 2-5 —Ä–∞–∑.'
  - '–ú–µ—Ç–æ–¥ QLoRA –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª–æ–∂–∏—Ç—å—Å—è –≤ –ø–∞–º—è—Ç—å –æ–±—ã—á–Ω–æ–π RTX 4090 (–∏ –¥–∞–∂–µ –º–µ–Ω—å—à–µ).'
  - '–ì–æ—Ç–æ–≤—ã–π –∫–æ–¥: –æ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è GGUF.'
published: true
---

<script>
  import PostCTA from '$entities/blog/ui/post-cta.svelte';
</script>

# –í–≤–µ–¥–µ–Ω–∏–µ

Llama 3 –æ—Ç Meta –ø–µ—Ä–µ–≤–µ—Ä–Ω—É–ª–∞ –∏–≥—Ä—É. –ú–æ–¥–µ–ª—å –Ω–∞ 8 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (8B) —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ –∏ —É–º–Ω–µ–µ, —á–µ–º Llama 2 70B. –ù–æ "–∏–∑ –∫–æ—Ä–æ–±–∫–∏" –æ–Ω–∞ –∑–Ω–∞–µ—Ç –≤—Å—ë –æ–±–æ –≤—Å—ë–º –ø–æ–Ω–µ–º–Ω–æ–≥—É. –ß—Ç–æ–±—ã –º–æ–¥–µ–ª—å –∑–∞–≥–æ–≤–æ—Ä–∏–ª–∞ –≤ —Å—Ç–∏–ª–µ –≤–∞—à–µ–≥–æ –±—Ä–µ–Ω–¥–∞, –Ω–∞—É—á–∏–ª–∞—Å—å –ø–∏—Å–∞—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–π –∫–æ–¥ –∏–ª–∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –µ—ë –Ω—É–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å (**Fine-Tuning**).

–†–∞–Ω—å—à–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–ª–∏—Å—å –∫–ª–∞—Å—Ç–µ—Ä—ã A100. –°–µ–≥–æ–¥–Ω—è, –±–ª–∞–≥–æ–¥–∞—Ä—è –º–µ—Ç–æ–¥–∞–º **QLoRA** –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ **Unsloth**, –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –Ω–∞ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–µ RTX 4090 –∑–∞ –ø–∞—Ä—É —á–∞—Å–æ–≤.

–í —ç—Ç–æ–º –≥–∞–π–¥–µ –º—ã –ø—Ä–æ–π–¥–µ–º –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å: –æ—Ç "–≥–æ–ª–æ–≥–æ" —Å–µ—Ä–≤–µ—Ä–∞ –¥–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.

## –ü–æ—á–µ–º—É Unsloth?

–ú—ã –Ω–µ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Hugging Face `Trainer` –≤ —á–∏—Å—Ç–æ–º –≤–∏–¥–µ. –ú—ã –≤–æ–∑—å–º–µ–º **Unsloth**.
–≠—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –≤—Ä—É—á–Ω—É—é –ø–µ—Ä–µ–ø–∏—Å–∞–ª–∞ —è–¥—Ä–∞ PyTorch –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏.

**–ß—Ç–æ —ç—Ç–æ –¥–∞–µ—Ç:**

1.  **–°–∫–æ—Ä–æ—Å—Ç—å:** –û–±—É—á–µ–Ω–∏–µ –∏–¥–µ—Ç –≤ 2-5 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ.
2.  **–ü–∞–º—è—Ç—å:** –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ VRAM —Å–Ω–∏–∂–∞–µ—Ç—Å—è –Ω–∞ 60%.
3.  **–ö–∞—á–µ—Å—Ç–≤–æ:** 0% –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ —Ç–∞ –∂–µ, –ø—Ä–æ—Å—Ç–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ).

## –®–∞–≥ 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è

–í–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è —Å–µ—Ä–≤–µ—Ä —Å GPU NVIDIA (–º–∏–Ω–∏–º—É–º 12 –ì–ë VRAM, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º 24 –ì–ë –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã).
–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞: Linux (Ubuntu 20.04/22.04).

–£—Å—Ç–∞–Ω–æ–≤–∏–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:

```bash
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Pytorch (–µ—Å–ª–∏ –µ—â–µ –Ω–µ—Ç)
pip install --upgrade pip
pip install "unsloth[colab-new] @ git+[https://github.com/unslothai/unsloth.git](https://github.com/unslothai/unsloth.git)"
pip install --no-deps "xformers<0.0.26" trl peft accelerate bitsandbytes
```

## –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `train.py` –∏ –Ω–∞—á–Ω–∏—Ç–µ —Å –∏–º–ø–æ—Ä—Ç–∞. –ú—ã –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ 4-–±–∏—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (Load in 4bit), —á—Ç–æ–±—ã –æ–Ω–∞ –∑–∞–Ω–∏–º–∞–ª–∞ –≤—Å–µ–≥–æ ~6 –ì–ë –ø–∞–º—è—Ç–∏.

```python
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048 # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 8192)
dtype = None # Auto detection (Float16 –¥–ª—è Tesla T4, Bfloat16 –¥–ª—è Ampere+)
load_in_4bit = True # –í–∫–ª—é—á–∞–µ–º 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ

# –ó–∞–≥—Ä—É–∂–∞–µ–º Llama-3 8B
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
```

## –®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA (–∞–¥–∞–ø—Ç–µ—Ä–æ–≤)

–ú—ã –Ω–µ –±—É–¥–µ–º –ø–µ—Ä–µ—É—á–∏–≤–∞—Ç—å –≤—Å–µ 8 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –≤–µ—Å–æ–≤ (—ç—Ç–æ –¥–æ–ª–≥–æ –∏ –¥–æ—Ä–æ–≥–æ). –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º **LoRA (Low-Rank Adaptation)**. –≠—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –ø–æ–≤–µ—Ä—Ö –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏.

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # –†–∞–Ω–≥ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º —É–º–Ω–µ–µ –∞–¥–∞–ø—Ç–µ—Ä, –Ω–æ —Ç—è–∂–µ–ª–µ–µ). 16, 32, 64 - –æ–∫.
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Unsloth —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç 0 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    bias = "none",
    use_gradient_checkpointing = "unsloth", # –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)
```

## –®–∞–≥ 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤–æ–∑—å–º–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç **Alpaca**, –Ω–æ –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ–Ω–∏—Ç—å –µ–≥–æ –Ω–∞ —Å–≤–æ–π JSON-—Ñ–∞–π–ª.
–§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–∞–∫–∏–º:

- **Instruction:** –í–æ–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–¥–∞—á–∞.
- **Input:** –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ).
- **Output:** –ñ–µ–ª–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏.

```python
from datasets import load_dataset

# –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
dataset = load_dataset("yahma/alpaca-cleaned", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True)
```

## –®–∞–≥ 4: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑—É–µ–º `SFTTrainer` (Supervised Fine-tuning Trainer) –æ—Ç Hugging Face.

<div class="my-6 border-l-4 border-blue-500 bg-blue-500/10 p-4">
  <p class="font-bold text-blue-400 m-0">üí° –°–æ–≤–µ—Ç –ø–æ –ø–∞–º—è—Ç–∏</p>
  <p class="mt-2 text-slate-300">
    –ï—Å–ª–∏ —É –≤–∞—Å –≤—ã–ª–µ—Ç–∞–µ—Ç –æ—à–∏–±–∫–∞ <code>OOM</code> (Out Of Memory), —É–º–µ–Ω—å—à–∏—Ç–µ <code>batch_size</code> —Å 2 –¥–æ 1 –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ <code>max_seq_length</code>.
  </p>
</div>

```python
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # –ú–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å True –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60, # –î–ª—è —Ç–µ—Å—Ç–∞ —Ö–≤–∞—Ç–∏—Ç 60 —à–∞–≥–æ–≤. –î–ª—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ - —Å—Ç–∞–≤—å—Ç–µ num_train_epochs = 1
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit", # 8-–±–∏—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —ç–∫–æ–Ω–æ–º–∏—Ç –∫—É—á—É –ø–∞–º—è—Ç–∏!
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

# –ü–û–ï–•–ê–õ–ò! üöÄ
trainer_stats = trainer.train()
```

## –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (Inference)

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫ –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.

```python
# –í–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–±—ã—Å—Ç—Ä–µ–µ)
FastLanguageModel.for_inference(model)

inputs = tokenizer(
[
    alpaca_prompt.format(
        "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —á–∏—Å–µ–ª –§–∏–±–æ–Ω–∞—á—á–∏.", # Instruction
        "", # Input
        "", # Output - –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–º, –º–æ–¥–µ–ª—å –¥–æ–ø–æ–ª–Ω–∏—Ç
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
print(tokenizer.batch_decode(outputs))
```

## –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

–í—ã –º–æ–∂–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–¥–∞–ø—Ç–µ—Ä—ã (LoRA adapters) ‚Äî –æ–Ω–∏ –≤–µ—Å—è—Ç –≤—Å–µ–≥–æ ~100 –ú–ë.

```python
model.save_pretrained("lora_model") # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
# model.push_to_hub("your_name/lora_model") # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face
```

–ò–ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ **GGUF** –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ –Ω–æ—É—Ç–±—É–∫–µ —á–µ—Ä–µ–∑ Ollama –∏–ª–∏ LM Studio:

```python
# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ 4-–±–∏—Ç–Ω–æ–º GGUF (q4_k_m)
model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
```

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ú—ã —Ç–æ–ª—å–∫–æ —á—Ç–æ –¥–æ–æ–±—É—á–∏–ª–∏ –æ–¥–Ω—É –∏–∑ —Å–∞–º—ã—Ö –º–æ—â–Ω—ã—Ö LLM —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–¥ –Ω–∞ 50 —Å—Ç—Ä–æ–∫ –∏ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–ª–∞—Å—Å–∞.

–í–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–Ω—è–ª –±—ã –Ω–∞ RTX 4090 –æ–∫–æ–ª–æ 15-20 –º–∏–Ω—É—Ç (–¥–ª—è 60 —à–∞–≥–æ–≤) –∏–ª–∏ –æ–∫–æ–ª–æ 2 —á–∞—Å–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É Alpaca.

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ?**

1.  –°–æ–±–µ—Ä–∏—Ç–µ —Å–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç (–¥–∏–∞–ª–æ–≥–∏ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏, –∫–æ–¥, —Å—Ç–∞—Ç—å–∏).
2.  –ó–∞–≥—Ä—É–∑–∏—Ç–µ –µ–≥–æ –≤–º–µ—Å—Ç–æ Alpaca.
3.  –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç–µ —Å–≤–æ–µ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.

<PostCTA
  title="–ù—É–∂–Ω–∞ –º–æ—â–Ω–∞—è –∫–∞—Ä—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è?"
  description="–†–∞–∑–≤–µ—Ä–Ω–∏—Ç–µ —Å—Ä–µ–¥—É —Å RTX 4090 –∏ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º Unsloth –∑–∞ 2 –º–∏–Ω—É—Ç—ã."
  btnText="–ó–∞–ø—É—Å—Ç–∏—Ç—å Jupyter Notebook"
/>
