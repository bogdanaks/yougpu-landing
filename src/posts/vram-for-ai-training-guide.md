---
slug: "vram-for-ai-training-guide"
title: "Сколько VRAM нужно для обучения нейросетей?"
description: "Почему 24 ГБ мало для обучения, куда исчезает память и как запустить LLaMA на слабом железе. Разбираем формулы, цифры и методы оптимизации (QLoRA, Checkpointing)."
date: "2026-01-21"
tags: ["Machine Learning", "LLM", "VRAM", "Fine-tuning", "NVIDIA", "Hardware"]
tldr:
  - "Обучение требует в 3-4 раза больше памяти, чем инференс: кроме весов нужно хранить градиенты и состояния оптимизатора."
  - "RTX 3090/4090 (24GB) хватит максимум для дообучения 7B/13B моделей с использованием квантования (QLoRA)."
  - "Если памяти не хватает: используйте Mixed Precision, Gradient Checkpointing и Batch Size = 1 с аккумуляцией градиентов."
published: true
---

<script>
  import PostCTA from '$entities/blog/ui/post-cta.svelte';
</script>

# Сколько видеопамяти (VRAM) реально нужно для обучения нейросетей? Большой гайд

Классическая ситуация: вы скачали веса LLaMA-7B, запускаете скрипт дообучения (finetuning) на своей любимой RTX 3060, и через 3 секунды консоль "радует" вас ошибкой:
`RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB...`

Знакомо? Ошибка **CUDA OOM (Out Of Memory)** — это главный враг ML-инженера. В отличие от CPU, где можно просто докинуть планку RAM или уйти в swap (потеряв производительность, но не процесс), с GPU всё жестче. Не влезло — процесс умер.

В этой статье разберем анатомию потребления памяти, почему для обучения нужно в 4 раза больше VRAM, чем для инференса, и как с этим жить, если у вас нет бюджета OpenAI.

---

## 1. Математика прожорливости: Куда уходит память?

Многие новички считают линейно: "Модель весит 14 ГБ, у меня карта на 16 ГБ. Значит, влезет".
**Спойлер: Нет.**

При обучении (Training) видеопамять расходуется на четыре основных компонента. Веса модели — это лишь верхушка айсберга.

### Анатомия VRAM

1.  **Веса модели (Model Weights):** Сами параметры нейросети.
2.  **Градиенты (Gradients):** Для каждого параметра нужно хранить значение градиента при обратном распространении ошибки (Backpropagation).
3.  **Состояния оптимизатора (Optimizer States):** Самый неочевидный и прожорливый пункт. Популярные оптимизаторы (вроде AdamW) хранят дополнительные копии параметров (моменты, дисперсию) для точной настройки.
4.  **Активации (Activations):** Промежуточные данные, генерируемые на каждом слое при прямом проходе (Forward pass). Их нужно хранить для вычисления градиентов. Зависит от **Batch Size** и длины последовательности (Sequence Length).

### Сколько байт весит один параметр?

Это зависит от точности (Precision). Стандарт де-факто сейчас — Mixed Precision (смешанная точность).

| Компонент                 | Точность                                 | Память на 1 параметр (Байты) |
| :------------------------ | :--------------------------------------- | :--------------------------- |
| **Вес (Parameter)**       | FP16 / BF16                              | **2 байта**                  |
| **Градиент**              | FP16 / BF16                              | **2 байта**                  |
| **Оптимизатор (Adam)**    | FP32 (копия весов + momentum + variance) | **12 байт**                  |
| **ИТОГО (Static Memory)** | Mixed Precision Setup                    | **~16 байт**                 |

> **Warning:** Эта таблица не учитывает активации! Активации могут занимать от 20% до 60% всей памяти в зависимости от архитектуры модели и размера батча.

---

## 2. Формула "на салфетке": Как быстро прикинуть объем?

Чтобы понять, потянет ли ваша карта **Full Fine-Tuning** (полное дообучение всех слоев), используйте это простое эмпирическое правило:

> **Необходимая VRAM ≈ (Количество параметров × 18 байт) + Активации**

Откуда 18 байт? Это 16 байт на "статику" (веса + оптимизатор + градиенты) плюс запас на буферы.

**Пример расчета для модели 7B (7 миллиардов параметров):**

1.  **Статика:** 7 млрд × 16 байт ≈ **112 ГБ**.
2.  **Плюс активации:** еще 10-20 ГБ (зависит от длины контекста).
3.  **Итог:** Вам нужно около **120-130 ГБ VRAM**.

**Шок-контент:** Чтобы обучить "малышку" LLaMA-7B по-честному (Full Fine-Tune), вам не хватит даже одной A100 80GB. Нужно минимум две, а лучше кластер.

Именно поэтому на бытовых картах (и даже на одиночных серверных) используют методы оптимизации, о которых ниже.

---

## 3. LLM и Трансформеры: Реальные цифры

Давайте спустимся с небес на землю. Если мы не OpenAI, мы вряд ли будем делать Full Fine-Tuning. Мы будем использовать LoRA (Low-Rank Adaptation) или просто запускать инференс.

Вот сводная таблица требований к VRAM для разных сценариев (приблизительные данные для контекста 4096 токенов):

| Модель (Размер)        | Inference (FP16) | Inference (INT4/GGUF) | Обучение (LoRA + QLoRA) | Обучение (Full FT) |
| :--------------------- | :--------------- | :-------------------- | :---------------------- | :----------------- |
| **7B (Mistral/Llama)** | ~14-16 GB        | ~5-6 GB               | **~8-12 GB**            | ~120 GB            |
| **13B (Llama 2)**      | ~26-28 GB        | ~9-10 GB              | **~16-24 GB**           | ~220 GB            |
| **34B (Yi/CodeLlama)** | ~70 GB           | ~20-22 GB             | **~32-40 GB**           | Cluster required   |
| **70B (Llama 3)**      | ~140 GB          | ~40-48 GB             | **~48-80 GB**           | Cluster required   |

> **Вывод:** Если у вас есть **RTX 3090/4090 (24GB)**, ваш потолок для комфортного обучения — это 7B или 13B модели с использованием квантования (QLoRA). Для 70B вам уже нужен серверный уровень.

---

## 4. Если памяти не хватает: Техники оптимизации

Если `torch.cuda.OutOfMemoryError` все еще преследует вас, у нас есть арсенал техник для борьбы с ним.

### A. Mixed Precision (FP16/BF16)

Это база. Мы не обучаем в FP32 (float), это медленно и дорого.
В PyTorch это делается через `torch.cuda.amp` или, если вы используете библиотеку HuggingFace Accelerate, это настраивается одной командой:

```bash
accelerate config
# На вопрос "Do you wish to use FP16 or BF16?" отвечаем "bf16" (если карта Ampere+) или "fp16".
```

### B. Gradient Checkpointing (Activation Checkpointing)

**Суть:** Мы не храним все промежуточные активации в памяти. Вместо этого мы пересчитываем их "на лету" во время обратного прохода (backward pass).
**Эффект:** Снижает потребление памяти от активаций в 5-10 раз.
**Цена:** Обучение замедляется на 20-30%.

Как включить в HuggingFace Transformers:

```python
model.gradient_checkpointing_enable()
```

### C. LoRA и QLoRA

Это то, что позволило запускать обучение LLM дома.

- **LoRA:** Мы замораживаем веса основной модели (они не требуют градиентов и состояний оптимизатора) и обучаем маленькие матрицы-адаптеры.
- **QLoRA:** Мы еще и сжимаем основную модель до 4 бит.

**Результат:** Потребление памяти падает с 120 ГБ до 10-12 ГБ для модели 7B.

### D. Batch Size & Gradient Accumulation

Если у вас не влезает Batch Size = 32, ставьте Batch Size = 1.
Чтобы математика обучения не пострадала, используйте **Gradient Accumulation** (накопление градиентов). Вы делаете 32 шага с батчем 1, суммируете градиенты и только потом делаете шаг оптимизатора.

```python
# Пример конфига TrainingArguments
training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=32, # Эффективный батч = 32
    fp16=True,
    ...
)
```

---

## 5. Выбор железа: Consumer vs Enterprise

Многие инженеры пытаются собрать "ферму" из игровых карт. Это работает до определенного предела.

### Почему RTX 4090 — не панацея?

1.  **Объем VRAM:** 24 ГБ — это "стеклянный потолок". Вы не обучите 70B модель даже с QLoRA без танцев с бубном (CPU Offloading, который дико медленный).
2.  **P2P (Peer-to-Peer) связь:** Игровые карты NVIDIA "порезаны" по скорости обмена данными между собой. NVLink на 4090 фактически мертв. Обучение на 2x4090 не дает прироста x2, а часто просто тормозит из-за пересылки данных по PCIe.

### Когда нужны серверные карты (A6000, A100, H100)?

- **A6000 / A6000 Ada (48GB):** Золотая середина. Влезает Llama-3-70B (QLoRA) или Mixtral 8x7B. Можно тренировать 13B модели с большим контекстом.
- **A100 (80GB):** Король скорости. Память HBM2e в разы быстрее GDDR6X. Это критично для больших батчей.

Если ваша задача выходит за рамки "поиграться с 7B на вечер", экономически выгоднее [арендовать мощный GPU сервер](https://console.yougpu.ru) на пару часов, чем покупать железо за $2000+, которое будет устаревать. На [yougpu.ru](https://console.yougpu.ru) можно взять машину с A6000 или A100, быстро прогнать обучение и выключить.

---

## 6. Чек-лист: Что делать при ошибке OOM (Troubleshooting)

Вы запустили скрипт, и все упало. Ваш алгоритм действий:

1.  **Уменьшить `per_device_train_batch_size`**. Попробуйте 1. Если заработало — повышайте `gradient_accumulation_steps`.
2.  **Включить Gradient Checkpointing**. Это часто спасает ситуацию без потери качества модели.
3.  **Использовать 8-bit Optimizer**. Библиотека `bitsandbytes` позволяет использовать оптимизатор Adam, который ест в 4 раза меньше памяти.
    ```python
    import bitsandbytes as bnb
    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=1e-5)
    ```
4.  **Очистить кэш (редко помогает, но можно пробовать)**. В Python коде:
    ```python
    import torch
    torch.cuda.empty_cache()
    import gc
    gc.collect()
    ```
5.  **Сдаться и масштабироваться**. Если вы уперлись в лимиты QLoRA и Batch Size 1, значит, задача переросла ваше железо. Идите на [yougpu.ru](https://console.yougpu.ru) и берите карту с 48+ ГБ памяти.

---

## Заключение

Видеопамять в Deep Learning — это ресурс, которым нужно управлять. Нельзя просто "заливать проблему деньгами", покупая бесконечные карты, но и пытаться обучать GPT-4 на ноутбуке — утопия.

Для старта и экспериментов с 7B моделями вам хватит 12-16 ГБ VRAM и техник QLoRA. Для серьезной работы с моделями уровня 70B или длинными контекстами (RAG, суммаризация книг) — ориентируйтесь на 48-80 ГБ VRAM.

<PostCTA
  title="Надоело видеть CUDA out of memory?"
  description="Ваша локальная карта не тянет Full Fine-Tuning? Арендуйте сервер с NVIDIA A6000 (48 ГБ) или A100 (80 ГБ). Готовое окружение для ML — просто запустите код."
  btnText="Забрать сервер с 80GB VRAM"
/>
